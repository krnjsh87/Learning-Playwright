# Day 24: Advanced CI/CD Pipelines, DevOps Integration & Production Deployment

**Date:** Day 24 of 25
**Duration:** 8 hours
**Difficulty:** Advanced
**Focus Area:** Advanced CI/CD Pipelines, Kubernetes for Test Automation, Docker Orchestration, Production Deployment Strategies, Observability & Monitoring, Test Infrastructure at Scale

---

## ğŸ¯ **Learning Objectives**

By the end of Day 24, you will:

âœ… Design and implement advanced CI/CD pipelines for test automation at enterprise scale.
âœ… Master Docker Compose for multi-service test environments.
âœ… Understand Kubernetes basics and deployment of test infrastructure.
âœ… Implement GitOps principles for infrastructure as code.
âœ… Set up comprehensive monitoring and observability for test environments.
âœ… Configure distributed test execution across multiple runners.
âœ… Implement blue-green and canary deployment strategies.
âœ… Build automated infrastructure provisioning for test labs.
âœ… Create self-healing test environments.
âœ… Prepare for production-level automation architecture.

---

## â° **Daily Schedule (8 Hours)**

| Time | Activity | Duration |
|------|----------|----------|
| 8:00 - 8:30 | Review Day 23 & Enterprise CI/CD Introduction | 30 min |
| 8:30 - 10:30 | **Theory Session 1:** Advanced CI/CD Architecture & DevOps | 2 hours |
| 10:30 - 11:00 | Break | 30 min |
| 11:00 - 1:00 PM | **Hands-On Lab 1:** Multi-Service Docker & Compose Setup | 2 hours |
| 1:00 - 2:00 PM | Lunch Break | 1 hour |
| 2:00 - 4:00 PM | **Theory Session 2:** Kubernetes, GitOps & Infrastructure as Code | 2 hours |
| 4:00 - 4:30 PM | Break | 30 min |
| 4:30 - 6:30 PM | **Hands-On Lab 2:** Production Deployment & Observability | 2 hours |

---

## ğŸ“š **THEORY SESSION 1: Advanced CI/CD Architecture & DevOps (2 hours)**

### **Part 24.1: Enterprise CI/CD Pipeline Architecture**

#### **CI/CD Pipeline Maturity Levels**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CI/CD MATURITY MODEL                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚ LEVEL 1: BASIC CI/CD                                       â”‚
â”‚ â”œâ”€ Manual testing                                          â”‚
â”‚ â”œâ”€ Basic build automation                                  â”‚
â”‚ â”œâ”€ No automated test execution                             â”‚
â”‚ â””â”€ Manual deployment                                       â”‚
â”‚                                                            â”‚
â”‚ LEVEL 2: AUTOMATED TESTING                                 â”‚
â”‚ â”œâ”€ Automated unit tests                                    â”‚
â”‚ â”œâ”€ Basic integration tests                                 â”‚
â”‚ â”œâ”€ Scheduled test runs                                     â”‚
â”‚ â””â”€ Semi-automated deployment                               â”‚
â”‚                                                            â”‚
â”‚ LEVEL 3: CONTINUOUS INTEGRATION                            â”‚
â”‚ â”œâ”€ All tests automated (unit, integration, E2E)            â”‚
â”‚ â”œâ”€ Tests run on every commit                               â”‚
â”‚ â”œâ”€ Code coverage reporting                                 â”‚
â”‚ â”œâ”€ Multiple environments                                   â”‚
â”‚ â””â”€ Automated rollback capability                           â”‚
â”‚                                                            â”‚
â”‚ LEVEL 4: CONTINUOUS DEPLOYMENT                             â”‚
â”‚ â”œâ”€ Automated testing across multiple browsers              â”‚
â”‚ â”œâ”€ Performance & load testing automated                    â”‚
â”‚ â”œâ”€ Automated security scanning                             â”‚
â”‚ â”œâ”€ Blue-green deployments                                  â”‚
â”‚ â”œâ”€ Canary releases                                         â”‚
â”‚ â””â”€ Real-time monitoring & alerting                         â”‚
â”‚                                                            â”‚
â”‚ LEVEL 5: CONTINUOUS DELIVERY + DEVOPS                      â”‚
â”‚ â”œâ”€ Full infrastructure as code                             â”‚
â”‚ â”œâ”€ Zero-downtime deployments                               â”‚
â”‚ â”œâ”€ Distributed test execution at scale                     â”‚
â”‚ â”œâ”€ Self-healing infrastructure                             â”‚
â”‚ â”œâ”€ ML-based anomaly detection                              â”‚
â”‚ â”œâ”€ Chaos engineering & resilience testing                  â”‚
â”‚ â””â”€ Advanced observability & distributed tracing            â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Advanced Pipeline Stages**

**Stage 1: Source Control Triggers**
```yaml
# GitHub repository events that trigger pipeline
on:
  push:
    branches: [main, develop, staging]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'package.json'
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
  schedule:
    - cron: '0 2 * * *'  # Nightly tests
    - cron: '0 */4 * * *' # Every 4 hours
```

**Stage 2: Build & Dependency Management**
```yaml
# Automated build with caching
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
      - uses: actions/cache@v3
        with:
          path: node_modules
          key: ${{ runner.os }}-npm-${{ hashFiles('package-lock.json') }}
      - run: npm ci
      - run: npm run build
      - run: npm run lint
```

**Stage 3: Security Scanning**
```yaml
# SAST, dependency check, secret scanning
  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
      - name: Run OWASP dependency check
        uses: dependency-check/Dependency-Check_Action@main
```

**Stage 4: Automated Testing Pipeline**
```yaml
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - run: npm run test:unit -- --coverage
      - run: npm run test:unit:watch  # For CI
  
  integration-tests:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
      redis:
        image: redis:7
    steps:
      - run: npm run test:integration
  
  e2e-tests-parallel:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        shard: [1, 2, 3, 4]
    steps:
      - run: npx playwright test --shard=${{ matrix.shard }}
  
  performance-tests:
    runs-on: ubuntu-latest
    steps:
      - run: k6 run scripts/load-test.js
```

**Stage 5: Quality Gates**
```yaml
  quality-gates:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests]
    steps:
      - name: Check code coverage threshold
        run: |
          COVERAGE=$(cat coverage/coverage-summary.json | jq '.total.lines.pct')
          if (( $(echo "$COVERAGE < 80" | bc -l) )); then
            echo "Coverage $COVERAGE% is below 80% threshold"
            exit 1
          fi
      - name: Check SonarQube quality gate
        run: |
          curl -s https://sonarqube.company.com/api/qualitygates/project_status \
            -u ${{ secrets.SONAR_TOKEN }}: | jq .projectStatus.status
```

**Stage 6: Artifact Management**
```yaml
  artifacts:
    runs-on: ubuntu-latest
    needs: [unit-tests, e2e-tests]
    steps:
      - name: Upload test reports
        uses: actions/upload-artifact@v3
        with:
          name: test-reports
          path: |
            coverage/
            playwright-report/
            junit-reports/
      - name: Upload to artifact repository
        run: |
          npm run build
          npx npm-publish --registry ${{ secrets.ARTIFACTORY_URL }}
```

**Stage 7: Deployment Strategies**
```yaml
  deploy-staging:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: staging
    steps:
      - name: Deploy to staging
        run: |
          kubectl apply -f k8s/staging/ --kubeconfig=${{ secrets.KUBECONFIG }}
      - name: Run smoke tests
        run: npm run test:smoke -- --baseUrl=https://staging.app.com
  
  deploy-production:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
      - name: Blue-green deployment
        run: ./scripts/deploy-bluegreen.sh
      - name: Canary deployment (5% traffic)
        run: ./scripts/deploy-canary.sh 0.05
      - name: Monitor metrics
        run: ./scripts/monitor-sla.sh
```

### **Part 24.2: Distributed Test Execution & Scaling**

#### **Test Sharding Strategies**

**Strategy 1: File-Based Sharding**
```javascript
// playwright.config.ts
export default {
  // Shard tests across multiple workers
  webServer: {
    command: 'npm run dev',
    url: 'http://localhost:3000'
  },
  
  // Run tests in parallel
  workers: 4,
  
  // Shard configuration
  fullyParallel: true,
  
  // For CI/CD sharding
  projects: [
    {
      name: 'chromium',
      use: { ...devices['Desktop Chrome'] },
      testDir: './tests',
      grep: /(@shard1)/,
    },
  ],
};
```

**Strategy 2: Test Suite Sharding**
```yaml
# GitHub Actions matrix strategy
strategy:
  matrix:
    shard: [1, 2, 3, 4, 5, 6, 7, 8]
    include:
      - shard: 1
        test-path: 'tests/auth/**'
      - shard: 2
        test-path: 'tests/dashboard/**'
      - shard: 3
        test-path: 'tests/api/**'
      - shard: 4
        test-path: 'tests/payment/**'
      # ... more shards

steps:
  - name: Run shard ${{ matrix.shard }}
    run: |
      npx playwright test ${{ matrix.test-path }} \
        --shard=${{ matrix.shard }}/8
```

**Strategy 3: Dynamic Test Allocation**
```javascript
// distribute-tests.js
const fs = require('fs');
const glob = require('glob');

function distributeTests(numShards) {
  const testFiles = glob.sync('tests/**/*.test.ts');
  const shards = Array.from({ length: numShards }, () => []);
  
  // Distribute by file size (heavier tests first)
  testFiles.sort((a, b) => {
    const sizeA = fs.statSync(a).size;
    const sizeB = fs.statSync(b).size;
    return sizeB - sizeA;
  });
  
  // Distribute round-robin to balance load
  testFiles.forEach((file, index) => {
    shards[index % numShards].push(file);
  });
  
  return shards;
}

// Generate shard configuration
const shards = distributeTests(8);
shards.forEach((files, index) => {
  console.log(`Shard ${index + 1}: ${files.length} files`);
});
```

#### **Cross-Platform & Multi-Browser Testing at Scale**

```typescript
// playwright.config.ts - Enterprise Scale
import { defineConfig, devices } from '@playwright/test';

export default defineConfig({
  testDir: './tests',
  
  // Global configuration
  globalSetup: './tests/global-setup.ts',
  globalTeardown: './tests/global-teardown.ts',
  
  // Retry failed tests
  retries: process.env.CI ? 2 : 0,
  
  // Workers for parallelization
  workers: process.env.CI ? 1 : 4,
  
  // Timeout configuration
  timeout: 60000,
  expect: { timeout: 10000 },
  
  // Reporter configuration
  reporter: [
    ['html', { outputFolder: 'html-report' }],
    ['json', { outputFile: 'test-results.json' }],
    ['junit', { outputFile: 'junit-results.xml' }],
    ['github', { printSteps: true }],
  ],
  
  // Projects for different browsers
  projects: [
    {
      name: 'chromium-desktop',
      use: {
        ...devices['Desktop Chrome'],
        // Chrome-specific options
        headless: true,
        args: ['--disable-gpu', '--no-sandbox'],
      },
      timeout: 60000,
    },
    {
      name: 'firefox-desktop',
      use: {
        ...devices['Desktop Firefox'],
      },
    },
    {
      name: 'webkit-desktop',
      use: {
        ...devices['Desktop Safari'],
      },
    },
    {
      name: 'chromium-mobile',
      use: {
        ...devices['Pixel 5'],
      },
    },
    {
      name: 'webkit-iphone',
      use: {
        ...devices['iPhone 12'],
      },
    },
  ],
  
  // Use webServer for serving application
  webServer: {
    command: 'npm run dev',
    url: 'http://localhost:3000',
    reuseExistingServer: !process.env.CI,
  },
});
```

### **Part 24.3: Infrastructure as Code & GitOps**

#### **GitOps Principles**

```
GitOps is a paradigm where:

1. ALL infrastructure & configuration is in Git
2. Git is the single source of truth
3. Changes are made through pull requests
4. Automated agents sync desired state with actual state
5. Rollback is as simple as reverting a commit

GitOps Benefits:
âœ“ Version control for all infrastructure changes
âœ“ Audit trail of all modifications
âœ“ Easy rollbacks (just revert commits)
âœ“ Consistent environments (dev, staging, prod)
âœ“ Team collaboration through code review
âœ“ Disaster recovery (rebuild from Git)
```

#### **Example GitOps Repository Structure**

```
infrastructure-as-code/
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ values.yaml           # Dev-specific values
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml    # Dev customizations
â”‚   â”‚   â””â”€â”€ secrets-sealed.yaml   # Encrypted secrets
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ values.yaml
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ secrets-sealed.yaml
â”‚   â””â”€â”€ prod/
â”‚       â”œâ”€â”€ values.yaml
â”‚       â”œâ”€â”€ kustomization.yaml
â”‚       â””â”€â”€ secrets-sealed.yaml
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â””â”€â”€ kustomization.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ decrypt-secrets.sh
â”‚   â”œâ”€â”€ apply-env.sh
â”‚   â””â”€â”€ validate-manifests.sh
â””â”€â”€ README.md
```

---

## ğŸ›  **HANDS-ON LAB 1: Multi-Service Docker & Compose Setup (2 hours)**

### **Objective**

Build a comprehensive Docker Compose environment for testing a microservices application with Playwright tests, API mocking, and database services.

### **Step-by-Step Guide**

#### **Step 1: Create Docker Compose Configuration**

Create `docker-compose.yml` in your project root:

```yaml
version: '3.9'

# Define custom networks
networks:
  test-network:
    driver: bridge

# Define reusable build context
x-build: &build
  context: .
  dockerfile: Dockerfile

x-env: &env
  NODE_ENV: test
  LOG_LEVEL: debug

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: test-postgres
    environment:
      POSTGRES_USER: testuser
      POSTGRES_PASSWORD: testpass123
      POSTGRES_DB: testdb
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U testuser"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - test-network

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: test-redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - test-network

  # Mock API Server
  mock-api:
    <<: *build
    container_name: test-mock-api
    command: node scripts/mock-server.js
    environment:
      <<: *env
      PORT: 3001
      POSTGRES_URL: postgres://testuser:testpass123@postgres:5432/testdb
      REDIS_URL: redis://redis:6379
    ports:
      - "3001:3001"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - test-network
    volumes:
      - .:/app
      - /app/node_modules

  # Application Under Test
  app:
    <<: *build
    container_name: test-app
    command: npm run dev
    environment:
      <<: *env
      PORT: 3000
      API_BASE_URL: http://mock-api:3001
    ports:
      - "3000:3000"
    depends_on:
      mock-api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - test-network
    volumes:
      - .:/app
      - /app/node_modules

  # Playwright Test Runner
  playwright:
    <<: *build
    container_name: test-playwright
    command: npx playwright test
    environment:
      <<: *env
      BASE_URL: http://app:3000
      CI: "true"
    depends_on:
      app:
        condition: service_healthy
    networks:
      - test-network
    volumes:
      - .:/app
      - /app/node_modules
      - ./playwright-report:/app/playwright-report
    profiles:
      - test

  # Selenium Grid (Optional - for additional cross-browser testing)
  selenium-hub:
    image: selenium/hub:4
    container_name: test-selenium-hub
    ports:
      - "4444:4444"
    networks:
      - test-network

  chrome:
    image: selenium/node-chrome:4
    container_name: test-chrome
    depends_on:
      - selenium-hub
    environment:
      SE_EVENT_BUS_HOST: selenium-hub
      SE_EVENT_BUS_PUBLISH_PORT: 4442
      SE_EVENT_BUS_SUBSCRIBE_PORT: 4443
    networks:
      - test-network
    profiles:
      - selenium

volumes:
  postgres_data:

```

#### **Step 2: Create Initialization Script**

Create `init-db.sql`:

```sql
-- Create tables for test database
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  email VARCHAR(255) UNIQUE NOT NULL,
  username VARCHAR(100) NOT NULL,
  password_hash VARCHAR(255) NOT NULL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE posts (
  id SERIAL PRIMARY KEY,
  user_id INTEGER REFERENCES users(id),
  title VARCHAR(255) NOT NULL,
  content TEXT,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE comments (
  id SERIAL PRIMARY KEY,
  post_id INTEGER REFERENCES posts(id),
  user_id INTEGER REFERENCES users(id),
  content TEXT NOT NULL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Insert test data
INSERT INTO users (email, username, password_hash) VALUES
  ('test@example.com', 'testuser', 'hashed_password_123'),
  ('admin@example.com', 'admin', 'hashed_password_456');

INSERT INTO posts (user_id, title, content) VALUES
  (1, 'First Post', 'This is my first post'),
  (1, 'Second Post', 'Another interesting post'),
  (2, 'Admin Post', 'Post from admin user');
```

#### **Step 3: Create Dockerfile**

Create `Dockerfile`:

```dockerfile
FROM node:18-alpine

WORKDIR /app

# Install system dependencies
RUN apk add --no-cache curl postgresql-client

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci

# Copy source code
COPY . .

# Expose port
EXPOSE 3000

# Default command
CMD ["npm", "start"]
```

#### **Step 4: Create Docker Compose Overrides for CI/CD**

Create `docker-compose.ci.yml`:

```yaml
version: '3.9'

services:
  postgres:
    environment:
      POSTGRES_INITDB_ARGS: "--encoding=UTF8"
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=256MB"
      - "-c"
      - "max_connections=200"

  app:
    build:
      context: .
      dockerfile: Dockerfile
      cache_from:
        - node:18-alpine
    environment:
      NODE_ENV: test
      DATABASE_URL: postgresql://testuser:testpass123@postgres:5432/testdb

  playwright:
    build:
      context: .
      dockerfile: Dockerfile.playwright
    environment:
      HEADLESS: "true"
      BASE_URL: http://app:3000
```

Create `Dockerfile.playwright`:

```dockerfile
FROM mcr.microsoft.com/playwright:v1.39.0-jammy

WORKDIR /app

COPY package*.json ./
RUN npm ci

COPY . .

ENTRYPOINT ["npx", "playwright", "test"]
CMD []
```

#### **Step 5: Create Orchestration Scripts**

Create `scripts/docker-up.sh`:

```bash
#!/bin/bash
set -e

echo "ğŸ³ Starting Docker Compose services..."

# Build images
docker-compose build

# Start services
docker-compose up -d

# Wait for services to be healthy
echo "â³ Waiting for services to be healthy..."
docker-compose ps

# Run database migrations (if any)
docker-compose exec app npm run migrate

# Run seeds
docker-compose exec postgres psql -U testuser -d testdb -f /docker-entrypoint-initdb.d/init.sql

echo "âœ… All services are up and healthy!"
```

Create `scripts/docker-test.sh`:

```bash
#!/bin/bash
set -e

echo "ğŸ§ª Running tests in Docker..."

# Run tests
docker-compose run --rm playwright

# Generate reports
docker-compose exec -T app npm run test:report

echo "ğŸ“Š Test reports generated in playwright-report/"
```

Create `scripts/docker-down.sh`:

```bash
#!/bin/bash
echo "ğŸ›‘ Stopping Docker Compose services..."
docker-compose down -v
echo "âœ… All services stopped and volumes removed"
```

#### **Step 6: Create GitHub Actions Workflow Using Docker Compose**

Create `.github/workflows/docker-compose-tests.yml`:

```yaml
name: Docker Compose Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  docker-tests:
    runs-on: ubuntu-latest
    
    services:
      # Services defined in docker-compose
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: testuser
          POSTGRES_PASSWORD: testpass123
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Build Docker images
        run: docker-compose -f docker-compose.yml -f docker-compose.ci.yml build
      
      - name: Start services
        run: |
          docker-compose -f docker-compose.yml -f docker-compose.ci.yml up -d
          sleep 10
      
      - name: Check service health
        run: |
          docker-compose ps
          docker-compose exec -T postgres pg_isready -U testuser
      
      - name: Run migrations
        run: docker-compose exec -T app npm run migrate
      
      - name: Run tests
        run: docker-compose run --rm playwright
      
      - name: Upload reports
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: playwright-report
          path: playwright-report/
      
      - name: Stop services
        if: always()
        run: docker-compose down -v
```

#### **Step 7: Usage Commands**

```bash
# Start all services
bash scripts/docker-up.sh

# Run tests
bash scripts/docker-test.sh

# Run specific test
docker-compose run --rm playwright -- tests/login.spec.ts

# View logs
docker-compose logs -f app
docker-compose logs -f postgres

# Execute shell in container
docker-compose exec app bash
docker-compose exec postgres psql -U testuser -d testdb

# Clean up
bash scripts/docker-down.sh

# For CI/CD
docker-compose -f docker-compose.yml -f docker-compose.ci.yml up --abort-on-container-exit
```

### **Mini-Assignment**

1. **Extend the Docker Compose setup** to include Elasticsearch and Kibana for log aggregation.
2. **Create health check scripts** that validate all services are running correctly.
3. **Implement network policies** to restrict communication between services.
4. **Add monitoring with Prometheus** for resource usage metrics.

---

## ğŸ“š **THEORY SESSION 2: Kubernetes, GitOps & Infrastructure as Code (2 hours)**

### **Part 24.4: Kubernetes Fundamentals for Test Infrastructure**

#### **Kubernetes Architecture for Testing**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  KUBERNETES CLUSTER                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Control Plane (Master Nodes)                     â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ â€¢ API Server                                      â”‚  â”‚
â”‚  â”‚ â€¢ Controller Manager                              â”‚  â”‚
â”‚  â”‚ â€¢ Scheduler                                       â”‚  â”‚
â”‚  â”‚ â€¢ etcd (state storage)                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Worker 1    â”‚  â”‚  Worker 2    â”‚  â”‚  Worker 3    â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ Kubelet      â”‚  â”‚ Kubelet      â”‚  â”‚ Kubelet      â”‚ â”‚
â”‚  â”‚ Container    â”‚  â”‚ Container    â”‚  â”‚ Container    â”‚ â”‚
â”‚  â”‚ Runtime      â”‚  â”‚ Runtime      â”‚  â”‚ Runtime      â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚
â”‚  â”‚ â”‚   Pod    â”‚â”‚  â”‚ â”‚   Pod    â”‚â”‚  â”‚ â”‚   Pod    â”‚â”‚ â”‚
â”‚  â”‚ â”‚ Containerâ”‚â”‚  â”‚ â”‚ Containerâ”‚â”‚  â”‚ â”‚ Containerâ”‚â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚
â”‚  â”‚ â”‚   Pod    â”‚â”‚  â”‚ â”‚   Pod    â”‚â”‚  â”‚ â”‚   Pod    â”‚â”‚ â”‚
â”‚  â”‚ â”‚ Containerâ”‚â”‚  â”‚ â”‚ Containerâ”‚â”‚  â”‚ â”‚ Containerâ”‚â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚  Persistent Volumes, Services, Ingresses              â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Kubernetes Deployment for Test Automation**

Create `k8s/deployment.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-runner
  namespace: test-automation
  labels:
    app: test-runner
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: test-runner
  template:
    metadata:
      labels:
        app: test-runner
        version: v1
    spec:
      # Pod disruption budgets for high availability
      terminationGracePeriodSeconds: 60
      
      # Init container to wait for dependencies
      initContainers:
      - name: wait-for-services
        image: busybox:1.28
        command: ['sh', '-c', 'until nc -z app:3000; do echo waiting for app; sleep 2; done;']
      
      containers:
      - name: playwright
        image: mcr.microsoft.com/playwright:v1.39.0-jammy
        imagePullPolicy: Always
        
        # Resource requests and limits
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        
        # Environment variables
        env:
        - name: BASE_URL
          value: "http://app:3000"
        - name: CI
          value: "true"
        - name: HEADLESS
          value: "true"
        - name: WORKERS
          value: "4"
        - name: NODE_ENV
          valueFrom:
            configMapKeyRef:
              name: test-config
              key: node-env
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: password
        
        # Volume mounts
        volumeMounts:
        - name: test-results
          mountPath: /test-results
        - name: config
          mountPath: /config
        
        # Liveness and readiness probes
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - test -f /tmp/test-running || exit 0
          initialDelaySeconds: 30
          periodSeconds: 10
        
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - "true"
          initialDelaySeconds: 10
          periodSeconds: 5
        
        # Security context
        securityContext:
          runAsNonRoot: true
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        
        # Startup probe
        startupProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - "true"
          failureThreshold: 3
          periodSeconds: 10
      
      # Volumes
      volumes:
      - name: test-results
        emptyDir: {}
      - name: config
        configMap:
          name: test-config
      
      # Pod disruption policy
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - test-runner
              topologyKey: kubernetes.io/hostname
      
      # Service account
      serviceAccountName: test-runner
      
      # Network policy
      dnsPolicy: ClusterFirst
```

#### **ConfigMap and Secrets**

Create `k8s/configmap.yaml`:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-config
  namespace: test-automation
data:
  node-env: "test"
  playwright-config: |
    {
      "retries": 2,
      "timeout": 60000,
      "workers": 4
    }
  test-env.sh: |
    export NODE_ENV=test
    export HEADLESS=true
    export WORKERS=4
---
apiVersion: v1
kind: Secret
metadata:
  name: db-secrets
  namespace: test-automation
type: Opaque
stringData:
  username: testuser
  password: testpass123
  url: postgresql://testuser:testpass123@postgres:5432/testdb
```

#### **Service and Ingress**

Create `k8s/service.yaml`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: test-runner-service
  namespace: test-automation
  labels:
    app: test-runner
spec:
  type: ClusterIP
  selector:
    app: test-runner
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-runner-ingress
  namespace: test-automation
spec:
  rules:
  - host: tests.company.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: test-runner-service
            port:
              number: 80
```

### **Part 24.5: Observability & Monitoring**

#### **Observability Stack (Prometheus + Grafana)**

Create `k8s/monitoring.yaml`:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    scrape_configs:
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
spec:
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus
        args:
          - '--config.file=/etc/prometheus/prometheus.yml'
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
      volumes:
      - name: config
        configMap:
          name: prometheus-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
spec:
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: admin
        volumeMounts:
        - name: storage
          mountPath: /var/lib/grafana
      volumes:
      - name: storage
        emptyDir: {}
```

#### **Distributed Tracing with Jaeger**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
spec:
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one
        ports:
        - containerPort: 6831
          protocol: UDP
        - containerPort: 16686
          protocol: TCP
        env:
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
```

---

## ğŸ›  **HANDS-ON LAB 2: Production Deployment & Observability (2 hours)**

### **Objective**

Implement blue-green and canary deployments with health checks, automated rollbacks, and comprehensive monitoring.

### **Step-by-Step Guide**

#### **Step 1: Implement Blue-Green Deployment Strategy**

Create `scripts/deploy-bluegreen.sh`:

```bash
#!/bin/bash
set -e

NAMESPACE="production"
SERVICE="test-automation"
NEW_VERSION=$1

if [ -z "$NEW_VERSION" ]; then
  echo "Usage: $0 <version>"
  exit 1
fi

echo "ğŸ”µ Starting Blue-Green Deployment for version $NEW_VERSION"

# Step 1: Get current (blue) deployment
BLUE_DEPLOY="${SERVICE}-blue"
GREEN_DEPLOY="${SERVICE}-green"
CURRENT=$(kubectl get service $SERVICE -n $NAMESPACE -o jsonpath='{.spec.selector.deployment}')

echo "Current active deployment: $CURRENT"

# Step 2: Determine which is active and which is inactive
if [ "$CURRENT" == "$BLUE_DEPLOY" ]; then
  ACTIVE=$BLUE_DEPLOY
  INACTIVE=$GREEN_DEPLOY
else
  ACTIVE=$GREEN_DEPLOY
  INACTIVE=$BLUE_DEPLOY
fi

echo "Active: $ACTIVE, Inactive: $INACTIVE"

# Step 3: Deploy new version to inactive (green) deployment
echo "ğŸŸ¢ Deploying version $NEW_VERSION to $INACTIVE..."
kubectl set image deployment/$INACTIVE \
  $SERVICE=$REGISTRY/$SERVICE:$NEW_VERSION \
  -n $NAMESPACE

# Step 4: Wait for deployment to be ready
echo "â³ Waiting for deployment to be ready..."
kubectl rollout status deployment/$INACTIVE -n $NAMESPACE --timeout=5m

# Step 5: Run smoke tests against new deployment
echo "ğŸ§ª Running smoke tests..."
INACTIVE_POD=$(kubectl get pod -l deployment=$INACTIVE -n $NAMESPACE -o jsonpath='{.items[0].metadata.name}')
kubectl exec -it $INACTIVE_POD -n $NAMESPACE -- npm run test:smoke

if [ $? -ne 0 ]; then
  echo "âŒ Smoke tests failed. Rollback not needed (new version was never activated)"
  exit 1
fi

# Step 6: Switch traffic to new deployment (green)
echo "ğŸ”„ Switching traffic to $INACTIVE..."
kubectl patch service $SERVICE -n $NAMESPACE \
  -p '{"spec":{"selector":{"deployment":"'$INACTIVE'"}}}'

# Step 7: Verify traffic is flowing
echo "âœ… Traffic switched. Monitoring metrics..."
sleep 10

# Step 8: Monitor error rates
ERROR_RATE=$(kubectl exec -it deployment/prometheus -n monitoring -- \
  promtool query instant 'rate(http_requests_total{status=~"5.."}[5m])')

if (( $(echo "$ERROR_RATE > 0.01" | bc -l) )); then
  echo "âš ï¸  High error rate detected. Rolling back..."
  kubectl patch service $SERVICE -n $NAMESPACE \
    -p '{"spec":{"selector":{"deployment":"'$ACTIVE'"}}}'
  exit 1
fi

# Step 9: Keep old version for quick rollback
echo "ğŸ‰ Blue-Green deployment successful!"
echo "Previous version ($ACTIVE) available for quick rollback"
```

#### **Step 2: Implement Canary Deployment**

Create `scripts/deploy-canary.sh`:

```bash
#!/bin/bash
set -e

NAMESPACE="production"
SERVICE="test-automation"
NEW_VERSION=$1
CANARY_PERCENT=${2:-5}  # Default 5% traffic

echo "ğŸš€ Starting Canary Deployment: $NEW_VERSION ($CANARY_PERCENT% traffic)"

# Step 1: Deploy canary version alongside production
echo "ğŸŸ¡ Creating canary deployment..."
kubectl set image deployment/${SERVICE}-canary \
  $SERVICE=$REGISTRY/$SERVICE:$NEW_VERSION \
  -n $NAMESPACE

# Step 2: Update Istio VirtualService for traffic splitting
echo "ğŸ”„ Configuring traffic split ($CANARY_PERCENT% to canary)..."
kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: $SERVICE
  namespace: $NAMESPACE
spec:
  hosts:
  - $SERVICE
  http:
  - match:
    - uri:
        prefix: "/"
    route:
    - destination:
        host: ${SERVICE}-stable
      weight: $((100 - CANARY_PERCENT))
    - destination:
        host: ${SERVICE}-canary
      weight: $CANARY_PERCENT
EOF

# Step 3: Monitor canary metrics
echo "ğŸ“Š Monitoring canary deployment..."
for i in {1..12}; do
  echo "Check $i/12 (5 minutes elapsed)..."
  
  # Get error rate for canary
  CANARY_ERROR=$(kubectl logs -l version=canary -n $NAMESPACE --tail=100 | \
    grep -c "ERROR" || echo "0")
  
  # Get error rate for stable
  STABLE_ERROR=$(kubectl logs -l version=stable -n $NAMESPACE --tail=100 | \
    grep -c "ERROR" || echo "0")
  
  CANARY_ERROR_RATE=$((CANARY_ERROR / (CANARY_ERROR + STABLE_ERROR + 1)))
  STABLE_ERROR_RATE=$((STABLE_ERROR / (CANARY_ERROR + STABLE_ERROR + 1)))
  
  echo "Canary error rate: ${CANARY_ERROR_RATE}%"
  echo "Stable error rate: ${STABLE_ERROR_RATE}%"
  
  # If canary error rate is significantly higher, rollback
  if [ $CANARY_ERROR_RATE -gt $((STABLE_ERROR_RATE + 5)) ]; then
    echo "âŒ Canary error rate too high. Rolling back..."
    kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: $SERVICE
  namespace: $NAMESPACE
spec:
  hosts:
  - $SERVICE
  http:
  - match:
    - uri:
        prefix: "/"
    route:
    - destination:
        host: ${SERVICE}-stable
      weight: 100
EOF
    exit 1
  fi
  
  sleep 30
done

# Step 4: Gradually increase canary traffic
echo "ğŸ“ˆ Gradually increasing traffic to canary..."
for PERCENT in 10 25 50 75 100; do
  echo "Shifting to $PERCENT%..."
  kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: $SERVICE
  namespace: $NAMESPACE
spec:
  hosts:
  - $SERVICE
  http:
  - match:
    - uri:
        prefix: "/"
    route:
    - destination:
        host: ${SERVICE}-stable
      weight: $((100 - PERCENT))
    - destination:
        host: ${SERVICE}-canary
      weight: $PERCENT
EOF
  sleep 60
done

# Step 5: Promote canary to stable
echo "âœ… Canary deployment successful. Promoting to stable..."
kubectl set image deployment/${SERVICE}-stable \
  $SERVICE=$REGISTRY/$SERVICE:$NEW_VERSION \
  -n $NAMESPACE

echo "ğŸ‰ Canary deployment completed successfully!"
```

#### **Step 3: Create Health Checks and Readiness Probes**

Create `k8s/health-checks.yaml`:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: health-check-scripts
data:
  liveness.sh: |
    #!/bin/bash
    # Check if test runner process is alive
    ps aux | grep playwright | grep -v grep
    exit $?
  
  readiness.sh: |
    #!/bin/bash
    # Check if application is ready to accept traffic
    curl -f http://localhost:3000/health
    exit $?
  
  startup.sh: |
    #!/bin/bash
    # Check if application completed initialization
    [ -f /tmp/initialized ]
    exit $?
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-runner-with-probes
spec:
  selector:
    matchLabels:
      app: test-runner
  template:
    metadata:
      labels:
        app: test-runner
    spec:
      containers:
      - name: app
        image: test-runner:latest
        
        # Startup Probe: Runs until success before other probes
        startupProbe:
          httpGet:
            path: /health/startup
            port: 3000
          failureThreshold: 30
          periodSeconds: 10
        
        # Readiness Probe: Can pod accept traffic?
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 3000
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 1
          failureThreshold: 3
        
        # Liveness Probe: Is pod alive? If not, restart
        livenessProbe:
          httpGet:
            path: /health/live
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
```

#### **Step 4: Set Up Automated Rollback**

Create `scripts/monitor-and-rollback.sh`:

```bash
#!/bin/bash

NAMESPACE="production"
SERVICE="test-automation"
ERROR_THRESHOLD=0.01  # 1% error rate
RESPONSE_TIME_THRESHOLD=5000  # 5 seconds

while true; do
  # Get current metrics
  ERROR_RATE=$(kubectl exec -it deployment/prometheus -n monitoring -- \
    promtool query instant 'rate(http_requests_total{status=~"5.."}[5m])' | \
    grep -oE '[0-9]+\.[0-9]+' | head -1)
  
  AVG_RESPONSE=$(kubectl exec -it deployment/prometheus -n monitoring -- \
    promtool query instant 'rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])' | \
    grep -oE '[0-9]+\.[0-9]+' | head -1)
  
  echo "Error Rate: $ERROR_RATE | Avg Response: ${AVG_RESPONSE}ms"
  
  # Check thresholds
  if (( $(echo "$ERROR_RATE > $ERROR_THRESHOLD" | bc -l) )); then
    echo "âŒ Error rate too high ($ERROR_RATE). Initiating rollback..."
    kubectl rollout undo deployment/$SERVICE -n $NAMESPACE
    kubectl rollout status deployment/$SERVICE -n $NAMESPACE --timeout=5m
    break
  fi
  
  if (( $(echo "$AVG_RESPONSE > $RESPONSE_TIME_THRESHOLD" | bc -l) )); then
    echo "âŒ Response time too high (${AVG_RESPONSE}ms). Initiating rollback..."
    kubectl rollout undo deployment/$SERVICE -n $NAMESPACE
    kubectl rollout status deployment/$SERVICE -n $NAMESPACE --timeout=5m
    break
  fi
  
  sleep 30
done
```

#### **Step 5: Create Comprehensive Monitoring Dashboard**

Create `k8s/grafana-dashboard.json`:

```json
{
  "dashboard": {
    "title": "Test Automation Infrastructure",
    "panels": [
      {
        "title": "Test Execution Rate",
        "targets": [
          {
            "expr": "rate(tests_executed_total[5m])"
          }
        ]
      },
      {
        "title": "Test Success Rate",
        "targets": [
          {
            "expr": "rate(tests_passed_total[5m]) / rate(tests_executed_total[5m])"
          }
        ]
      },
      {
        "title": "Pod Restart Count",
        "targets": [
          {
            "expr": "increase(kube_pod_container_status_restarts_total[1h])"
          }
        ]
      },
      {
        "title": "Resource Usage",
        "targets": [
          {
            "expr": "container_memory_usage_bytes{pod=~\"test-runner.*\"}"
          },
          {
            "expr": "container_cpu_usage_seconds_total{pod=~\"test-runner.*\"}"
          }
        ]
      }
    ]
  }
}
```

### **Mini-Assignment**

1. **Implement PodDisruptionBudgets** to maintain availability during node maintenance.
2. **Create custom metrics** for test execution time, failure rate, and resource usage.
3. **Set up automated alerts** for deployment failures and high error rates.
4. **Document rollback procedures** and test them in staging environment.

---

## ğŸ§  **Knowledge Check - Advanced CI/CD & DevOps**

### **MCQ Questions**

**Question 1:** Which CI/CD maturity level includes automated security scanning and canary deployments?
- A) Level 2
- B) Level 3
- C) Level 4
- D) Level 5
**Answer: C** - Continuous Deployment (Level 4)

**Question 2:** What is the primary benefit of GitOps?
- A) Faster deployments
- B) Git as single source of truth with automated sync
- C) Easier CI/CD setup
- D) Reduced storage costs
**Answer: B** - GitOps principle

**Question 3:** In blue-green deployment, what happens if tests fail on the new version (green)?
- A) Rollback immediately
- B) No traffic switch occurs
- C) Both versions serve traffic
- D) Data is lost
**Answer: B** - Green never becomes active if tests fail

**Question 4:** Which Kubernetes probe is used to determine if a pod should be restarted?
- A) Readiness probe
- B) Startup probe
- C) Liveness probe
- D) Health probe
**Answer: C** - Liveness probe triggers restart

**Question 5:** What is a sidecar container in Kubernetes?
- A) Container running on same node as main pod
- B) Additional container in same pod for logging, monitoring, etc.
- C) Service mesh proxy
- D) Storage volume
**Answer: B** - Sidecar pattern for auxiliary services

---

## ğŸ“‹ **Daily Checklist - Day 24**

### **Morning (Review & Theory)**
- [ ] Review Day 23 concepts (GitHub Actions, LambdaTest)
- [ ] Complete Theory Session 1 (Enterprise CI/CD Architecture)
- [ ] Take notes on GitOps principles
- [ ] Complete Theory Session 2 (Kubernetes & Infrastructure as Code)
- [ ] Review observability stack (Prometheus, Grafana, Jaeger)

### **Hands-On Practice**
- [ ] Set up Docker Compose multi-service environment
- [ ] Create initialization scripts for database and services
- [ ] Build and run Dockerfile for test runner
- [ ] Execute tests in Docker containers
- [ ] Implement blue-green deployment script
- [ ] Create canary deployment workflow
- [ ] Set up health checks and readiness probes
- [ ] Configure automated rollback mechanism

### **Assignments & Projects**
- [ ] Complete Mini-Assignment 1 (Docker Compose Extensions)
- [ ] Complete Mini-Assignment 2 (Kubernetes Deployment)
- [ ] Create GitHub Actions workflow for Docker Compose tests
- [ ] Document deployment procedures
- [ ] Write runbooks for incident response

### **Review & Reflection**
- [ ] Summarize key learnings from Day 24
- [ ] Identify areas needing more practice
- [ ] Review code and deployment scripts
- [ ] Prepare for Day 25 (Final Exam & Interview Prep)
- [ ] Update GitHub portfolio project with DevOps content

---

## ğŸ¯ **Key Takeaways - Day 24**

1. **Advanced CI/CD Pipelines** can be built with GitHub Actions, supporting complex workflows with caching, artifacts, and parallel jobs.

2. **Docker & Docker Compose** enable reproducible test environments with service orchestration, networking, and health checks.

3. **Kubernetes** provides enterprise-grade infrastructure for deploying test automation at scale with self-healing capabilities.

4. **GitOps** leverages Git as the single source of truth, enabling reproducible infrastructure and easy rollbacks.

5. **Deployment Strategies** (blue-green, canary) reduce risk and provide rollback capabilities for production releases.

6. **Observability** (Prometheus, Grafana, Jaeger) enables monitoring of test infrastructure and rapid issue identification.

7. **Infrastructure as Code** enables version control, review, and automation of infrastructure changes.

8. **Health Checks** (liveness, readiness, startup) ensure robust and self-healing test infrastructure.

---

## ğŸ“š **Resources for Further Learning**

### **Documentation**
- Docker Compose: https://docs.docker.com/compose/
- Kubernetes Basics: https://kubernetes.io/docs/concepts/overview/
- GitHub Actions: https://docs.github.com/en/actions
- Prometheus: https://prometheus.io/docs/

### **Tools & Platforms**
- Kubernetes Dashboard: https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
- Prometheus Operator: https://github.com/prometheus-operator/prometheus-operator
- Flux CD (GitOps): https://fluxcd.io/
- ArgoCD (GitOps): https://argoproj.github.io/cd/

### **Best Practices**
- Container security scanning with Trivy
- Service mesh with Istio for traffic management
- Policy as code with OPA/Gatekeeper
- Secrets management with Sealed Secrets or HashiCorp Vault

---

## ğŸ“ **Notes for Instructors**

**Teaching Tips:**
- Start with simple Docker Compose setup before introducing Kubernetes complexity.
- Use real-world examples from companies like Netflix, Amazon, and Google.
- Encourage hands-on experimentation with deployment strategies.
- Discuss trade-offs between complexity and benefits.
- Have students deploy to local k3s cluster for cost-effective learning.

**Common Pitfalls:**
- Overcomplicating infrastructure before mastering basics.
- Not monitoring deployments properly.
- Assuming all services are always healthy.
- Not having proper rollback procedures.
- Missing security considerations in container images.

---

**Next Step:** Proceed to Day 25 for Final Exam, Interview Preparation, and Career Advancement Strategy.
